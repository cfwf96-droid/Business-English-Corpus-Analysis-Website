from fastapi import FastAPI, Response
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import re
import os
import sys
from datetime import datetime

# æ­£åˆ™è¡¨è¾¾å¼æ¨¡å¼
chunk_mark_pattern = re.compile(r'\(\s*ROOT\s*\(', re.IGNORECASE)
tag_pattern = re.compile(r"([A-Za-z\-]+)\((.*?)\)", re.DOTALL | re.IGNORECASE)
content_part_pattern = re.compile(r"([A-Za-z]+)\s+([^\(\)\s]+)", re.DOTALL)
chinese_overlap_pattern = re.compile(r"([\u4e00-\u9fa5])ä¸€\1")

# æ”¯æŒå‘½ä»¤è¡ŒæŒ‡å®šç«¯å£
def get_port():
    for arg in sys.argv:
        if arg.startswith("--port"):
            try:
                return int(arg.split("=")[1])
            except:
                pass
    return 8000

APP_PORT = get_port()

# åˆå§‹åŒ–FastAPIæœåŠ¡
app = FastAPI(
    title=f"ä¸­æ–‡ç»„å—æ£€ç´¢APIï¼ˆç«¯å£ï¼š{APP_PORT}ï¼‰",
    version="4.3",
    description="å½»åº•è§£å†³ä¸­æ–‡ä¸‹è½½ç¼–ç é—®é¢˜"
)

# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ•°æ®æ¨¡å‹
class QueryRequest(BaseModel):
    query: str
    top_n: int = 10
    need_report: bool = False
    corpus_path: str = "68f850c1927c8.txt"

# æœåŠ¡å¥åº·æ£€æŸ¥æ¥å£
@app.get("/health", summary="æœåŠ¡å¥åº·æ£€æŸ¥")
def health_check():
    return {
        "status": "healthy",
        "port": APP_PORT,
        "time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "message": "Pythonç»„å—æ£€ç´¢æœåŠ¡è¿è¡Œæ­£å¸¸"
    }

# è¯­æ–™è¯»å–ä¸è§£æï¼ˆè¿”å›å®Œæ•´ç»„å—å­—å…¸ï¼‰
def read_corpus(corpus_path: str) -> tuple[list, dict]:
    """è¯»å–è¯­æ–™å¹¶è¿”å›(å¤„ç†åçš„è¯­æ–™åˆ—è¡¨, å®Œæ•´ç»„å—å­—å…¸{è¡Œå·: ç»„å—å†…å®¹})"""
    try:
        corpus_path = os.path.abspath(corpus_path)
        if not os.path.exists(corpus_path):
            raise FileNotFoundError(f"è¯­æ–™æ–‡ä»¶ä¸å­˜åœ¨ï¼è·¯å¾„ï¼š{corpus_path}")
        
        file_size_mb = round(os.path.getsize(corpus_path) / 1024 / 1024, 2)
        print(f"\n=== è¯­æ–™æ–‡ä»¶ä¿¡æ¯ ===")
        print(f"è·¯å¾„ï¼š{corpus_path}")
        print(f"å¤§å°ï¼š{file_size_mb} MB")
        print("==================\n")

        corpus = []
        full_chunks = {}  # å­˜å‚¨å®Œæ•´ç»„å—å†…å®¹ï¼ˆè¡Œå·: åŸå§‹å†…å®¹ï¼‰
        line_count = 0
        valid_count = 0

        with open(corpus_path, 'r', encoding='utf-8', errors='replace') as f:
            for line_num, line in enumerate(f, 1):
                line_count += 1
                line_strip = line.strip()
                full_chunks[line_num] = line_strip  # ä¿å­˜åŸå§‹ç»„å—å†…å®¹
                
                # æ¯1000è¡Œæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if line_count % 1000 == 0:
                    print(f"ğŸ”„ æ­£åœ¨å¤„ç†ç¬¬{line_count}è¡Œï¼Œå·²æ‰¾åˆ°{valid_count}æ¡æœ‰æ•ˆç»„å—")
                
                if not line_strip:
                    continue
                
                if not chunk_mark_pattern.search(line_strip):
                    continue
                
                # æå–çº¯æ–‡æœ¬å†…å®¹ï¼ˆç§»é™¤æ ‡ç­¾ï¼‰
                pure_text = re.sub(
                    r"\([A-Za-z\-]+\(.*?\)\)",
                    lambda m: m.group(0).split("(")[-1].split(")")[0] + " ",
                    line_strip
                )
                pure_text = re.sub(r"\s+", " ", pure_text).strip()
                
                corpus.append((line_num, line_strip, pure_text))
                valid_count += 1
        
        print(f"\nâœ… è¯­æ–™å¤„ç†å®Œæˆï¼æ€»è¡Œæ•°ï¼š{line_count}ï¼Œæœ‰æ•ˆç»„å—è¡Œæ•°ï¼š{valid_count}\n")

        if not corpus:
            raise ValueError(f"æ— æœ‰æ•ˆç»„å—ï¼è¯·ç¡®è®¤åŒ…å«'(ROOT('æ ‡è¯†")

        return corpus, full_chunks
    
    except Exception as e:
        print(f"è¯­æ–™è¯»å–é”™è¯¯ï¼š{str(e)}")
        raise

def parse_chunk(chunk_str: str) -> dict:
    """è§£æç»„å—å­—ç¬¦ä¸²ï¼Œæå–æ ‡ç­¾ä¿¡æ¯"""
    chunk_info = {"tags": {}, "full_text": chunk_str}
    try:
        tags = tag_pattern.findall(chunk_str)
        
        for match in tags:
            if len(match) != 2:
                print(f"âš ï¸  è¿‡æ»¤æ— æ•ˆæ ‡ç­¾åŒ¹é…ï¼š{match}ï¼ˆç»„å—ï¼š{chunk_str[:50]}...ï¼‰")
                continue
            
            tag_name, tag_content = match
            tag_name = tag_name.strip()
            tag_content = tag_content.strip()
            
            if not tag_name or not tag_content:
                continue
            
            content_parts = content_part_pattern.findall(tag_content)
            if not content_parts:
                continue
            
            if tag_name not in chunk_info["tags"]:
                chunk_info["tags"][tag_name] = []
            for pos, word in content_parts:
                chunk_info["tags"][tag_name].append({
                    "pos": pos,
                    "word": word,
                    "full_subchunk": f"({tag_name}({pos} {word}))"
                })
    
    except Exception as e:
        print(f"ç»„å—è§£æé”™è¯¯ï¼ˆ{chunk_str[:50]}...ï¼‰ï¼š{str(e)}")
    
    return chunk_info

def parse_query(query: str) -> dict:
    """è§£ææ£€ç´¢å¼ï¼Œç”Ÿæˆæ£€ç´¢æ¡ä»¶"""
    query = query.strip()
    if not query:
        raise ValueError("æ£€ç´¢å¼ä¸èƒ½ä¸ºç©º")
    
    query_info = {
        "raw_query": query,
        "type": "",
        "tags": [],
        "conditions": [],
        "intention": ""
    }

    # å•æ ‡ç­¾æ¨¡å¼ï¼ˆå¦‚ï¼šVP-PRD[vä¸€v]ï¼‰
    single_tag_pattern = re.compile(r"^([A-Za-z\-]+)\[([^\]]*)\]$")
    single_match = single_tag_pattern.match(query)
    if single_match:
        tag, cond_str = single_match.groups()
        query_info["type"] = "single_tag"
        query_info["tags"] = [tag]
        query_info["conditions"] = [_parse_condition(cond_str)]
        query_info["intention"] = _analyze_intention(query_info)
        return query_info

    # å¤šæ ‡ç­¾æ¨¡å¼ï¼ˆå¦‚ï¼šNP-SBJ[n]VP-PRD[v]ï¼‰
    multi_tag_pattern = re.compile(r"^([A-Za-z\-]+\[.*?\])+$")
    if multi_tag_pattern.match(query):
        tag_cond_pairs = re.findall(r"([A-Za-z\-]+)\[([^\]]*)\]", query)
        if tag_cond_pairs:
            query_info["type"] = "multi_tag"
            query_info["tags"] = [tag for tag, _ in tag_cond_pairs]
            query_info["conditions"] = [_parse_condition(cond) for _, cond in tag_cond_pairs]
            query_info["intention"] = _analyze_intention(query_info)
            return query_info

    # åµŒå¥—æ ‡ç­¾æ¨¡å¼ï¼ˆå¦‚ï¼šVP-PRD[NULL-MOD[d]VP-PRD[v]]ï¼‰
    nested_tag_pattern = re.compile(r"^([A-Za-z\-]+)\[(.*)\]$")
    nested_match = nested_tag_pattern.match(query)
    if nested_match and "[" in nested_match.group(2) and "]" in nested_match.group(2):
        outer_tag, inner_content = nested_match.groups()
        inner_query_info = parse_query(inner_content)
        query_info["type"] = "nested_tag"
        query_info["tags"] = [outer_tag] + inner_query_info["tags"]
        query_info["conditions"] = [_parse_condition("*")] + inner_query_info["conditions"]
        query_info["intention"] = _analyze_intention(query_info)
        return query_info

    raise ValueError(f"æ£€ç´¢å¼æ ¼å¼é”™è¯¯ï¼è¾“å…¥ï¼š{query}ï¼Œè¯·å‚è€ƒç¤ºä¾‹æ ¼å¼")

def _parse_condition(cond_str: str) -> dict:
    """è§£ææ£€ç´¢æ¡ä»¶"""
    cond_str = cond_str.strip()
    if not cond_str or cond_str == "*":
        return {"type": "any", "value": "*", "desc": "ä»»æ„å†…å®¹"}
    
    # é‡å æ¨¡å¼ï¼ˆå¦‚ï¼švä¸€vï¼‰
    pattern_match = re.match(r"^([a-z])ä¸€([a-z])$", cond_str)
    if pattern_match and pattern_match.group(1) == pattern_match.group(2):
        pos_type = pattern_match.group(1)
        pattern_desc_map = {
            "v": "åŠ¨è¯é‡å å¼ï¼ˆå¦‚ï¼šèµ°ä¸€èµ°ã€å¬ä¸€å¬ï¼‰",
            "n": "åè¯é‡å å¼ï¼ˆå¦‚ï¼šäººä¸€äººã€å¤©ä¸€å¤©ï¼‰",
            "a": "å½¢å®¹è¯é‡å å¼ï¼ˆå¦‚ï¼šçº¢ä¸€çº¢ã€äº®ä¸€äº®ï¼‰"
        }
        return {
            "type": "pattern",
            "value": cond_str,
            "pos": pos_type,
            "pattern_regex": chinese_overlap_pattern,
            "desc": pattern_desc_map.get(pos_type, f"{pos_type}ç±»è¯é‡å å¼")
        }
    
    # è¯æ€§æ¡ä»¶ï¼ˆå¦‚ï¼šnã€vã€dï¼‰
    if len(cond_str) <= 2 and cond_str.islower():
        pos_desc_map = {
            "d": "å‰¯è¯ï¼ˆå¦‚ï¼šå¾ˆã€éƒ½ã€ä¹Ÿï¼‰",
            "v": "åŠ¨è¯ï¼ˆå¦‚ï¼šèµ°ã€å¬ã€è¯´ï¼‰",
            "n": "åè¯ï¼ˆå¦‚ï¼šå¤©æ°”ã€ç»ç†ï¼‰",
            "a": "å½¢å®¹è¯ï¼ˆå¦‚ï¼šå¥½ã€åˆé€‚ï¼‰",
            "r": "ä»£è¯ï¼ˆå¦‚ï¼šæˆ‘ã€ä½ ï¼‰",
            "nr": "äººåï¼ˆå¦‚ï¼šç‹ç»ç†ï¼‰",
            "ns": "åœ°åï¼ˆå¦‚ï¼šåŒ—äº¬ï¼‰"
        }
        return {
            "type": "pos",
            "value": cond_str,
            "desc": pos_desc_map.get(cond_str, f"è¯æ€§({cond_str})")
        }
    
    # è¯è¯­æ¡ä»¶
    return {"type": "word", "value": cond_str, "desc": f"å…·ä½“è¯ï¼ˆ{cond_str}ï¼‰"}

def _analyze_intention(query_info: dict) -> str:
    """åˆ†ææ£€ç´¢æ„å›¾"""
    tag_desc_map = {
        "ROOT": "æ ¹èŠ‚ç‚¹",
        "IP": "ç‹¬ç«‹åˆ†å¥",
        "NP-SBJ": "ä¸»è¯­åè¯çŸ­è¯­",
        "VP-PRD": "è°“è¯­åŠ¨è¯çŸ­è¯­",
        "NULL-MOD": "å‰¯è¯ä¿®é¥°å—",
        "NP-OBJ": "å®¾è¯­åè¯çŸ­è¯­",
        "PP": "ä»‹è¯çŸ­è¯­"
    }
    if query_info["type"] == "single_tag":
        tag = query_info["tags"][0]
        cond = query_info["conditions"][0]
        tag_desc = tag_desc_map.get(tag, tag)
        return f"æ£€ç´¢{tag_desc}æ ‡ç­¾ä¸‹ï¼Œ{cond['desc']}çš„ç»„å—å†…å®¹"
    elif query_info["type"] == "multi_tag":
        tag_cond_pairs = list(zip(query_info["tags"], query_info["conditions"]))
        pairs_desc = "â†’".join([f"{tag_desc_map.get(tag, tag)}ï¼ˆ{cond['desc']}ï¼‰" for tag, cond in tag_cond_pairs])
        return f"æ£€ç´¢{ pairs_desc }çš„æ ‡ç­¾åºåˆ—å¯¹åº”çš„ç»„å—å†…å®¹"
    elif query_info["type"] == "nested_tag":
        outer_tag = query_info["tags"][0]
        inner_tags = query_info["tags"][1:]
        outer_desc = tag_desc_map.get(outer_tag, outer_tag)
        inner_desc = "â†’".join([tag_desc_map.get(tag, tag) for tag in inner_tags])
        return f"æ£€ç´¢{outer_desc}æ ‡ç­¾ä¸‹åµŒå¥—{inner_desc}æ ‡ç­¾åºåˆ—çš„ç»„å—å†…å®¹"
    return "æ£€ç´¢ç¬¦åˆæ¡ä»¶çš„ç»„å—å†…å®¹"

def match_structure(chunk_info: dict, query_info: dict) -> list:
    """åŒ¹é…ç»„å—ç»“æ„ä¸æ£€ç´¢æ¡ä»¶"""
    matched = []
    try:
        if query_info["type"] == "single_tag":
            tag = query_info["tags"][0]
            cond = query_info["conditions"][0]
            if tag not in chunk_info["tags"]:
                return []
            
            for item in chunk_info["tags"][tag]:
                pos, word, full_subchunk = item["pos"], item["word"], item["full_subchunk"]
                if (cond["type"] == "any") or \
                   (cond["type"] == "pos" and pos == cond["value"]) or \
                   (cond["type"] == "word" and word == cond["value"]) or \
                   (cond["type"] == "pattern" and pos == cond["pos"] and cond["pattern_regex"].search(word)):
                    matched.append({
                        "content": word,
                        "full_subchunk": full_subchunk
                    })
        
        elif query_info["type"] == "multi_tag":
            tag_cond_pairs = list(zip(query_info["tags"], query_info["conditions"]))
            if not all(tag in chunk_info["tags"] for tag, _ in tag_cond_pairs):
                return []
            
            tag_contents = {tag: chunk_info["tags"][tag] for tag, _ in tag_cond_pairs}
            max_len = min([len(contents) for contents in tag_contents.values()])
            
            for i in range(max_len):
                sequence_content = []
                sequence_subchunk = []
                valid = True
                
                for tag, cond in tag_cond_pairs:
                    item = tag_contents[tag][i]
                    pos, word, full_subchunk = item["pos"], item["word"], item["full_subchunk"]
                    
                    if not (cond["type"] == "any" or \
                           (cond["type"] == "pos" and pos == cond["value"]) or \
                           (cond["type"] == "word" and word == cond["value"]) or \
                           (cond["type"] == "pattern" and pos == cond["pos"] and cond["pattern_regex"].search(word))):
                        valid = False
                        break
                    
                    sequence_content.append(word)
                    sequence_subchunk.append(full_subchunk)
                
                if valid:
                    matched.append({
                        "content": "".join(sequence_content),
                        "full_subchunk": "".join(sequence_subchunk)
                    })
        
        elif query_info["type"] == "nested_tag":
            outer_tag = query_info["tags"][0]
            inner_query_info = {
                "type": "multi_tag" if len(query_info["tags"][1:]) > 1 else "single_tag",
                "tags": query_info["tags"][1:],
                "conditions": query_info["conditions"][1:]
            }
            
            if outer_tag not in chunk_info["tags"]:
                return []
            
            for item in chunk_info["tags"][outer_tag]:
                inner_chunk_str = item["full_subchunk"]
                inner_chunk_info = parse_chunk(inner_chunk_str)
                inner_matched = match_structure(inner_chunk_info, inner_query_info)
                matched.extend(inner_matched)
    
    except Exception as e:
        print(f"ç»“æ„åŒ¹é…é”™è¯¯ï¼š{str(e)}")
    
    return matched

def generate_report(results: list, full_chunks: dict, query_info: dict, corpus_path: str) -> str:
    """ç”Ÿæˆæ£€ç´¢æŠ¥å‘Šï¼ˆç¡®ä¿å…¼å®¹UTF-8ï¼‰"""
    try:
        # ç¡®ä¿æ‰€æœ‰å­—ç¬¦ä¸²éƒ½æ˜¯Unicodeï¼ˆé˜²æ­¢å­—èŠ‚ä¸²å¯¼è‡´ç¼–ç é—®é¢˜ï¼‰
        def ensure_str(s):
            if isinstance(s, bytes):
                return s.decode('utf-8', errors='replace')
            return str(s)
        
        report_lines = [
            "# ä¸­æ–‡ç»„å—æ£€ç´¢æŠ¥å‘Š",
            f"æ£€ç´¢æ—¶é—´ï¼š{ensure_str(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))}",
            f"è¯­æ–™è·¯å¾„ï¼š{ensure_str(corpus_path)}",
            f"æ£€ç´¢å¼ï¼š{ensure_str(query_info['raw_query'])}",
            f"æ£€ç´¢éœ€æ±‚ï¼š{ensure_str(query_info['intention'])}",
            f"åŒ¹é…ç»“æœæ€»æ•°ï¼š{len(results)}",
            "-" * 80,
            f"{'åºå·':<6}{'åŒ¹é…å†…å®¹':<15}{'ç»„å—è®°å½•å·':<12}{'åŒ¹é…éƒ¨åˆ†æ ‡çº¢çš„ç»„å—å†…å®¹'}",
            "-" * 80
        ]
        
        for idx, res in enumerate(results, 1):
            content = ensure_str(res["content"])
            line_num = res["line_num"]
            chunk_content = ensure_str(full_chunks.get(line_num, "æœªæ‰¾åˆ°ç»„å—å†…å®¹"))
            
            # æŠ¥å‘Šä¸­ç”¨ã€ã€‘æ ‡çº¢åŒ¹é…å†…å®¹
            highlighted_content = chunk_content.replace(content, f"ã€{content}ã€‘")
            
            # æ ¼å¼åŒ–è¾“å‡º
            idx_str = f"{idx}.".ljust(6)
            content_str = content.ljust(15)
            line_num_str = f"ç¬¬{line_num}è¡Œ".ljust(12)
            report_lines.append(f"{idx_str}{content_str}{line_num_str}{highlighted_content}")
        
        report_lines.append("-" * 80)
        return "\n".join(report_lines)
    except Exception as e:
        print(f"æŠ¥å‘Šç”Ÿæˆé”™è¯¯ï¼š{str(e)}")
        return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥ï¼š{str(e)}"

# APIæ¥å£
@app.post("/retrieve_chunk", summary="ç»„å—æ£€ç´¢")
def retrieve_chunk(request: QueryRequest):
    try:
        print(f"\nğŸ“¥ æ”¶åˆ°æ£€ç´¢è¯·æ±‚ï¼š{request.query}")
        
        # 1. è¯»å–è¯­æ–™ï¼ˆåŒæ—¶è·å–å®Œæ•´ç»„å—å­—å…¸ï¼‰
        corpus, full_chunks = read_corpus(request.corpus_path)
        
        # 2. è§£ææ£€ç´¢å¼
        query_info = parse_query(request.query)
        print(f"ğŸ” æ£€ç´¢éœ€æ±‚ï¼š{query_info['intention']}")
        
        # 3. åŒ¹é…ç»„å—
        results = []
        total_chunks = len(corpus)
        for idx, (line_num, chunk_str, pure_text) in enumerate(corpus, 1):
            # è¿›åº¦æç¤ºï¼ˆæ¯100è¡Œï¼‰
            if idx % 100 == 0:
                print(f"ğŸ”„ åŒ¹é…ç¬¬{idx}/{total_chunks}ä¸ªç»„å—ï¼Œå·²æ‰¾åˆ°{len(results)}æ¡ç»“æœ")
            
            # è§£æç»„å—
            chunk_info = parse_chunk(chunk_str)
            
            # åŒ¹é…ç»“æ„
            matched_items = match_structure(chunk_info, query_info)
            
            # å»é‡å¹¶æ·»åŠ ç»“æœ
            for item in matched_items:
                content = item["content"]
                if not any(r["content"] == content and r["line_num"] == line_num for r in results):
                    results.append({
                        "content": content,
                        "line_num": line_num,
                        "full_subchunk": item["full_subchunk"],
                        "pure_text": pure_text
                    })
        
        # 4. å‡†å¤‡è¿”å›ç»“æœ
        display_results = results[:request.top_n]
        query_info["result_count"] = len(results)
        
        # ç­›é€‰éœ€è¦è¿”å›çš„å®Œæ•´ç»„å—
        display_line_nums = [res["line_num"] for res in display_results]
        display_full_chunks = {
            line_num: full_chunks[line_num] 
            for line_num in display_line_nums 
            if line_num in full_chunks
        }
        
        print(f"âœ… æ£€ç´¢å®Œæˆï¼æ€»åŒ¹é…ï¼š{len(results)}æ¡\n")
        
        return {
            "status": "success",
            "query_info": query_info,
            "result_count": len(results),
            "display_count": len(display_results),
            "results": [{"content": r["content"], "line_num": r["line_num"]} for r in display_results],
            "full_chunks": display_full_chunks,
            "corpus_path": request.corpus_path,
            "report": generate_report(results, full_chunks, query_info, request.corpus_path) if request.need_report else ""
        }
    
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ æ£€ç´¢é”™è¯¯ï¼š{error_msg}\n")
        return {
            "status": "error",
            "detail": error_msg
        }

@app.post("/download_report", summary="ä¸‹è½½æŠ¥å‘Š")
def download_report(request: QueryRequest, response: Response):
    try:
        # è·å–æ£€ç´¢ç»“æœ
        retrieve_result = retrieve_chunk(request)
        if retrieve_result["status"] != "success" or not retrieve_result.get("report"):
            raise ValueError("æ— æ£€ç´¢ç»“æœï¼Œæ— æ³•ç”ŸæˆæŠ¥å‘Š")
        
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿æŠ¥å‘Šå†…å®¹æ˜¯UTF-8ç¼–ç çš„å­—èŠ‚æµ
        report_content = retrieve_result["report"]
        if isinstance(report_content, str):
            report_bytes = report_content.encode('utf-8')  # æ˜¾å¼ç¼–ç ä¸ºUTF-8
        else:
            report_bytes = report_content  # å·²ä¸ºå­—èŠ‚æµ
        
        filename = f"ä¸­æ–‡ç»„å—æ£€ç´¢æŠ¥å‘Š_{datetime.now().strftime('%YmdHis')}.txt"
        
        # è®¾ç½®å“åº”å¤´ï¼Œç¡®ä¿ä¸­æ–‡æ–‡ä»¶åå’ŒUTF-8ç¼–ç 
        response.headers["Content-Type"] = "text/plain; charset=utf-8"
        response.headers["Content-Length"] = str(len(report_bytes))
        # å¤„ç†ä¸­æ–‡æ–‡ä»¶åï¼ˆURLç¼–ç ï¼‰
        from urllib.parse import quote
        response.headers["Content-Disposition"] = f"attachment; filename*=UTF-8''{quote(filename)}"
        
        return Response(content=report_bytes, media_type="text/plain; charset=utf-8")
    
    except Exception as e:
        error_msg = str(e)
        print(f"\nâŒ ä¸‹è½½é”™è¯¯ï¼š{error_msg}\n")
        return {
            "status": "error",
            "detail": error_msg
        }

# å¯åŠ¨æœåŠ¡
if __name__ == "__main__":
    import uvicorn
    print("=" * 60)
    print(f"ğŸ“Œ ä¸­æ–‡ç»„å—æ£€ç´¢APIï¼ˆV4.3ï¼‰å¯åŠ¨æˆåŠŸ")
    print(f"æœåŠ¡ç«¯å£ï¼š{APP_PORT}")
    print(f"å¥åº·æ£€æŸ¥ï¼šhttp://127.0.0.1:{APP_PORT}/health")
    print(f"APIæ–‡æ¡£ï¼šhttp://127.0.0.1:{APP_PORT}/docs")
    print("æç¤ºï¼šä¿®æ”¹ç«¯å£éœ€åŒæ—¶æ›´æ–°PHPé¡µé¢ä¸­çš„ç«¯å£é…ç½®")
    print("=" * 60)
    uvicorn.run(
        app="chunk_analyzer:app",
        host="0.0.0.0",
        port=APP_PORT,
        reload=True,
        timeout_keep_alive=60
    )
